---
title: "New"
output: html_document
---
```{r}
cor.mtest <- function(mat, ...) {
    mat <- as.matrix(mat)
    n <- ncol(mat)
    p.mat<- matrix(NA, n, n)
    diag(p.mat) <- 0
    for (i in 1:(n - 1)) {
        for (j in (i + 1):n) {
            tmp <- cor.test(mat[, i], mat[, j], ...)
            p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
        }
    }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}
```

```{r}
#error metrics -- Confusion Matrix
err_metric=function(CM)
{
  TN =CM[1,1]
  TP =CM[2,2]
  FP =CM[1,2]
  FN =CM[2,1]
  precision =(TP)/(TP+FP)
  recall_score =(TP)/(TP+FN)
 
  f1_score=2*((precision*recall_score)/(precision+recall_score))
  accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
  False_positive_rate =(FP)/(FP+TN)
  False_negative_rate =(FN)/(FN+TP)
 
  print(paste("Precision value of the model: ",round(precision,2)))
  print(paste("Accuracy of the model: ",round(accuracy_model,2)))
  print(paste("Recall value of the model: ",round(recall_score,2)))
  print(paste("False Positive rate of the model: ",round(False_positive_rate,2)))
 
  print(paste("False Negative rate of the model: ",round(False_negative_rate,2)))
 
  print(paste("f1 score of the model: ",round(f1_score,2)))
}
```

```{r,message=F,warning=F}
library('tidyverse')
library('ggplot2')
library(DiffXTables)
library(lattice)
library(caret)
library(MLmetrics)
library(reticulate)
library(e1071)
library(rpart)
library(BBmisc)
library(reticulate)
library(pracma)
library(DMwR)
library(fastDummies)
library(stringr)
library(glmnet)
library(tensorflow)
library(keras)
library(BBmisc)
library(corrplot)
library(xgboost)
library(ROSE)
library(smotefamily)
library(UBL)
```

# 1. Reading and Understanding the Data
```{r}
churn_bigml_train = read.csv("churn-bigml-80.csv",head=T,sep = ",")
churn_bigml_test = read.csv("churn-bigml-20.csv",head=T,sep = ",")
feature_name = names(churn_bigml_train)
```

```{r}
# # Print the datatype of each column of the dataframe
str(churn_bigml_train)

head(churn_bigml_train)

# # let's look at the statistical aspects of the dataframe
summary(churn_bigml_train)
```
```{r}
# one-hot
for (i in feature_name){
  churn_bigml_train[[i]]=as.numeric(as.factor(churn_bigml_train[[i]]))
  churn_bigml_test[[i]]=as.numeric(as.factor(churn_bigml_test[[i]]))
}

churn_bigml_train$Churn = churn_bigml_train$Churn-1
churn_bigml_test$Churn = churn_bigml_test$Churn-1
```

```{r}
corrplot(cor(churn_bigml_train), method="color" )
corrplot(cor(churn_bigml_train), method="number" , number.cex=0.7 )
```
# 2. Data Cleaning

No missing value
```{r}
# Get the no of missing values in each column
for (i in feature_name){
    if (sum(churn_bigml_train[[i]] == ' ?')>0){
      data_adult[churn_bigml_train[[i]] == ' ?',][[i]] =NA
  }
}

colSums(is.na(churn_bigml_train))
```
# 3. Data Visualization/EDA

Analysis of the Continuous Variables
```{r}
no_num_var =c("State" ,'Churn', 'International.plan', 'Voice.mail.plan', 'Area.Code')
num_var =setdiff(feature_name, no_num_var)
```

```{r}
# Loop through each Continuous varible and plot the Boxplot
for (i in num_var){
  boxplot(churn_bigml_train[,i] ~ Churn, data = churn_bigml_train,ylab = i )
}
```
Let's cap the outliers to avoide any impact on the prediction of the models
```{r}
for (i in num_var){
  Q1 = quantile(churn_bigml_train[,i],probs =0.05)
  Q3 = quantile(churn_bigml_train[,i],probs =0.95)
  churn_bigml_train[churn_bigml_train[,i]<Q1,i] = Q1
  churn_bigml_train[churn_bigml_train[,i]>Q3,i] = Q3
}
```

```{r}
# # Loop through each Continuous varible and plot the Distplot
for (i in num_var){
hist(churn_bigml_train[,i] , breaks=40 ,prob = TRUE, col=rgb(0.2,0.8,0.5,0.5) , border=T,main='',xlab =i)
lines(density(churn_bigml_train[,i]))
}
```
Let's check the customer and churn distribution across the states
```{r}
ggplot(churn_bigml_train, aes(x = State)) +
  geom_bar()+
  theme(axis.text.x = element_text(angle=60, vjust=1, hjust=1))

ggplot(churn_bigml_train,aes(x=State,group=Churn,col=Churn,fill=Churn,))+
  geom_bar(alpha=0.2,stat="count")+
  geom_density(alpha=0.2)+
  theme(axis.text.x = element_text(angle=60, vjust=1, hjust=1))

ggplot(churn_bigml_train,aes(x=State,group=Churn,col=Churn,fill=Churn,))+
  geom_bar(alpha=0.2,position="dodge",stat="count")+
  geom_density(alpha=0.2)+
  theme(axis.text.x = element_text(angle=60, vjust=1, hjust=1))
```

# imbalance data
```{r}
churn_bigml_train%>%
  ggplot(aes(x=Churn))+
  geom_bar()+
  geom_text(stat='count',aes(label=..count..),vjust=1.6,color="white", size=3.5)

a = table(churn_bigml_train$Churn)
pie(a)
```

# 4. Feature Engineering
```{r}
churn_bigml_train['Total_call'] = churn_bigml_train['Total.day.calls']+
                                  churn_bigml_train['Total.eve.calls']+
                                  churn_bigml_train['Total.night.calls']+
                                  churn_bigml_train['Total.intl.calls']
churn_bigml_train['Total_minutes'] =  churn_bigml_train['Total.day.minutes']+
                                      churn_bigml_train['Total.eve.minutes']+         
                                      churn_bigml_train['Total.night.minutes']+ 
                                  churn_bigml_train['Total.intl.minutes']
churn_bigml_train['Total_charge'] = churn_bigml_train['Total.day.charge']+
                                    churn_bigml_train['Total.eve.charge']+
                                    churn_bigml_train['Total.night.charge']+
                                    churn_bigml_train['Total.intl.charge']

churn_bigml_test['Total_call'] = churn_bigml_test['Total.day.calls']+
                                  churn_bigml_test['Total.eve.calls']+
                                  churn_bigml_test['Total.night.calls']+
                                  churn_bigml_test['Total.intl.calls']
churn_bigml_test['Total_minutes'] =  churn_bigml_test['Total.day.minutes']+
                                      churn_bigml_test['Total.eve.minutes']+         
                                      churn_bigml_test['Total.night.minutes']+ 
                                      churn_bigml_test['Total.intl.minutes']
churn_bigml_test['Total_charge'] = churn_bigml_test['Total.day.charge']+
                                    churn_bigml_test['Total.eve.charge']+
                                    churn_bigml_test['Total.night.charge']+
                                    churn_bigml_test['Total.intl.charge']
```

```{r}
# Let's check for the highly correlated original variables that can be dropped
corrplot(cor(churn_bigml_train), method="color" )
corrplot(cor(churn_bigml_train), method="number" )
```

It can be seen that there is good correlation between the original variables and new features. Therefore, let's drop the following original variables

There is also a very high correlation (0.89) between Total minutes and Total charge. One of the can be dropped.
```{r}
churn_bigml_train['Total.day.calls'] = NULL 
churn_bigml_train['Total.eve.calls'] = NULL 
churn_bigml_train['Total.night.calls'] = NULL 
churn_bigml_train['Total.day.minutes'] = NULL 
churn_bigml_train['Total.eve.minutes'] = NULL 
churn_bigml_train['Total.night.minutes'] = NULL 
churn_bigml_train['Total.day.charge'] = NULL 
churn_bigml_train['Total.eve.charge'] = NULL 
churn_bigml_train['Total.night.charge'] = NULL 
churn_bigml_train['State'] = NULL

churn_bigml_test['Total.day.calls'] = NULL 
churn_bigml_test['Total.eve.calls'] = NULL 
churn_bigml_test['Total.night.calls'] = NULL 
churn_bigml_test['Total.day.minutes'] = NULL 
churn_bigml_test['Total.eve.minutes'] = NULL 
churn_bigml_test['Total.night.minutes'] = NULL 
churn_bigml_test['Total.day.charge'] = NULL 
churn_bigml_test['Total.eve.charge'] = NULL 
churn_bigml_test['Total.night.charge'] = NULL 
churn_bigml_test['State'] = NULL
```

# Normalize 
```{r}
index = which(names(churn_bigml_train) == 'Churn')
preproc1 <- preProcess(churn_bigml_train[,-index], method=c("center", "scale"))
churn_bigml_train[,-index]<- predict(preproc1, churn_bigml_train[,-index])

preproc1 <- preProcess(churn_bigml_test[,-index], method=c("center", "scale"))
churn_bigml_test[,-index]<- predict(preproc1, churn_bigml_test[,-index])
```

```{r}
high_value = quantile(churn_bigml_train[,'Total_charge'],probs =0.7)
high_value_cust = churn_bigml_train[churn_bigml_train[,'Total_charge']>=high_value,]
```

```{r}
trainlabel<-to_categorical(churn_bigml_train[,index])
testlabel<-to_categorical(churn_bigml_test[,index])
train =as.matrix(churn_bigml_train[,-index])
test =as.matrix(churn_bigml_test[,-index])
w = 2278/388
small_w = 388/(2278+388)
large_w = 2278/(2278+388)
```



```{r}
#weight = rep(1,dim(churn_bigml_train)[1])
#weight[churn_bigml_train$Churn==1] =w

logistic_regression = glm(Churn ~ .,data=churn_bigml_train , family='binomial' )
summary(logistic_regression)
logistic_predict=predict(logistic_regression, churn_bigml_test[,-index])
labels <- ifelse(logistic_predict > 0.5, 1, 0)
cm=table('real'=churn_bigml_test$Churn,'predict'=labels)
err_metric(cm)
```

# Over Sampling
improve the f1-score from 0.4 to 0.6 and the Accuracy not decline
```{r}
number_max = max(table(churn_bigml_train$Churn))
over <- ovun.sample(Churn ~ . , data = churn_bigml_train, method = "over" ,N =number_max*2)$data
logistic_regression = glm(Churn~. ,data=over , family='binomial')
summary(logistic_regression)
logistic_predict=predict(logistic_regression, churn_bigml_test[,-index])
labels <- ifelse(logistic_predict > 0.5, 1, 0)
cm=table('real'=churn_bigml_test$Churn,'predict'=labels)
err_metric(cm)
```

```{r}
number_min = min(table(churn_bigml_train$Churn))
under <- ovun.sample(Churn~., data=churn_bigml_train, method = "under", N = number_min*2)$data
logistic_regression = glm(Churn~. ,data=under , family='binomial')
# summary(logistic_regression)
logistic_predict=predict(logistic_regression, churn_bigml_test[,-10])
labels <- ifelse(logistic_predict > 0.5, 1, 0)
cm=table('real'=churn_bigml_test$Churn,'predict'=labels)
err_metric(cm)
```
# deep learning
```{r}
modeldeep <- keras_model_sequential()
modeldeep %>%
    layer_dense(units=32, activation = "relu",
              kernel_initializer = "he_normal",input_shape =c(dim(train)[2]))%>%
    layer_dropout(rate=0.2)%>%
    layer_dense(units=64, activation = "relu",
              kernel_initializer = "he_normal")%>%
    layer_dropout(rate=0.4)%>%
    layer_dense(units=128, activation = "relu",
              kernel_initializer = "he_normal")%>%
    layer_dropout(rate=0.4)%>%
    layer_dense(units=128, activation = "relu",
              kernel_initializer = "he_normal")%>%
    layer_dropout(rate=0.4)%>%
        layer_dense(units=2, activation = "sigmoid")

modeldeep %>%
  compile(loss="binary_crossentropy",
          optimizer="adam",
          metric="accuracy")
summary(modeldeep)
```

```{r}
history<- modeldeep %>%
  fit(train,trainlabel,batch_size=5, validation_split=0.2 ,epochs =50)

pred_new <- modeldeep%>%
  predict(test)%>% `>` (0.5)
pred_new = as.numeric(pred_new[,2])
cm=table('real'=testlabel[,2],'predict'=pred_new)
err_metric(cm)
```

```{r}
modeldeep1 <- keras_model_sequential()
modeldeep1 %>%
    layer_dense(units=32, activation = "relu",
              kernel_initializer = "he_normal",input_shape =c(dim(train)[2]))%>%
    layer_dropout(rate=0.2)%>%
    layer_dense(units=64, activation = "relu",
              kernel_initializer = "he_normal")%>%
    layer_dropout(rate=0.2)%>%
    layer_dense(units=128, activation = "relu",
              kernel_initializer = "he_normal")%>%
    layer_dropout(rate=0.4)%>%
    layer_dense(units=128, activation = "relu",
              kernel_initializer = "he_normal")%>%
    layer_dropout(rate=0.4)%>%
        layer_dense(units=2, activation = "sigmoid")
modeldeep1 %>%
  compile(loss="binary_crossentropy",
          optimizer="SGD",
          metric=c("accuracy"))
summary(modeldeep1)
```

```{r}
inputs <- layer_input(shape = c(dim(train)[2]))
predictions <- inputs %>%
    layer_dense(units=32, activation = "relu",
              kernel_initializer = "he_normal")%>%
    layer_dropout(rate=0.2)%>%
    layer_dense(units=64, activation = "relu",
              kernel_initializer = "he_normal")%>%
    layer_dropout(rate=0.4)%>%
    layer_dense(units=128, activation = "relu",
              kernel_initializer = "he_normal")%>%
    layer_dense(units=128, activation = "relu",
              kernel_initializer = "he_normal")%>%
    layer_dropout(rate=0.4)%>%
        layer_dense(units=2, activation = "sigmoid")
model <- keras_model(inputs = inputs, outputs = predictions)
model %>% compile(
  optimizer = 'SGD',
  loss = 'binary_crossentropy',
  metrics = c('accuracy')
)
summary(model)
```


```{r}
# new model
history1<- model %>%
  fit(train,trainlabel,batch_size=5, validation_split=0.2 , class_weight=list("0"=small_w,"1"=large_w),epochs =50)

pred_new <- model%>%
  predict(test)%>% `>` (0.5)
pred_new = as.numeric(pred_new[,2])
cm=table('real'=testlabel[,2],'predict'=pred_new)
err_metric(cm)

# old model
history1<- modeldeep1 %>%
  fit(train,trainlabel,batch_size=5, validation_split=0.2 , class_weight=list("0"=1,"1"=w),epochs =50)

pred <- modeldeep1%>%
  predict(test)%>% `>` (0.5)
pred = as.numeric(pred[,2])
cm=table('real'=testlabel[,2],'predict'=pred)
err_metric(cm)

```

```{r}
file = 'modeldeep1.h5'
save_model_hdf5(modeldeep1,file)
model = load_model_hdf5(file)
```

```{r}
accuracy = c(0.86,0.82,0.82)
f1_sore = c(0.14,0.48,0.49)
mycols = c("tan", "orange1")
score = matrix(c(accuracy,f1_sore),c(2,3),byrow =T)
rownames(score) = c('accuracy' , 'f1_sore')
barplot(score,beside = TRUE ,names.arg = c("None", "Over", "Under" ),main="score", legend = TRUE ,col=mycols,xlim = c(0, 12))
```

